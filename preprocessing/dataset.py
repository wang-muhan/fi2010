import torch
from torch.utils import data
import pytorch_lightning as pl
from torch.utils.data import DataLoader
import numpy as np
import constants as cst
import time
from torch.utils import data
from utils.utils_data import one_hot_encoding_type, tanh_encoding_type

class Dataset(data.Dataset):
    """Characterizes a dataset for PyTorch

    Supports optional precomputed start indices (to avoid crossing segments)
    and optional stock_ids per sample.
    """
    FI_2010 = "FI_2010"

    def __init__(self, x, y, seq_size, indices=None, stock_labels=None):
        """Initialization

        Args:
            x: time-major feature matrix [T, F]
            y: labels aligned to starting index (length T - seq_size + 1)
            seq_size: sequence length
            indices: optional 1D list/array/tensor of valid start positions
            stock_labels: optional 1D list/array/tensor of stock ids aligned to indices
        """
        self.seq_size = seq_size
        self.x = x
        self.y = y
        if isinstance(self.x, np.ndarray):
            self.x = torch.from_numpy(self.x).float()
        if isinstance(self.y, np.ndarray):
            self.y = torch.from_numpy(self.y).long()

        if indices is None:
            self.indices = torch.arange(self.y.shape[0], dtype=torch.long)
        else:
            self.indices = torch.as_tensor(indices, dtype=torch.long)

        if stock_labels is not None:
            stock_labels = torch.as_tensor(stock_labels, dtype=torch.long)
            if stock_labels.shape[0] != self.indices.shape[0]:
                raise ValueError("stock_labels must match indices length")
            self.stock_labels = stock_labels
        else:
            self.stock_labels = None

        self.length = self.indices.shape[0]
        self.data = self.x

    def __len__(self):
        """Denotes the total number of samples"""
        return self.length

    def __getitem__(self, i):
        start_idx = self.indices[i].item()
        input = self.x[start_idx:start_idx + self.seq_size, :]
        y_val = self.y[start_idx]
        if self.stock_labels is None:
            return input, y_val
        return input, y_val, self.stock_labels[i]

class DataModule(pl.LightningDataModule):
    def   __init__(self, train_set, val_set, batch_size, test_batch_size,  is_shuffle_train=True, test_set=None, num_workers=16):
        super().__init__()

        self.train_set = train_set
        self.val_set = val_set
        self.test_set = test_set
        self.batch_size = batch_size
        self.test_batch_size = test_batch_size
        self.is_shuffle_train = is_shuffle_train
        if train_set.data.device.type != cst.DEVICE:       #this is true only when we are using a GPU but the data is still on the CPU
            self.pin_memory = True
        else:
            self.pin_memory = False
        self.num_workers = num_workers

    def train_dataloader(self):
        return DataLoader(
            dataset=self.train_set,
            batch_size=self.batch_size,
            shuffle=self.is_shuffle_train,
            pin_memory=self.pin_memory,
            drop_last=False,
            num_workers=self.num_workers,
            persistent_workers=True
        )

    def val_dataloader(self):
        return DataLoader(
            dataset=self.val_set,
            batch_size=self.batch_size,
            shuffle=False,
            pin_memory=self.pin_memory,
            drop_last=False,
            num_workers=self.num_workers,
            persistent_workers=True
        )
    
    def test_dataloader(self):
        return DataLoader(
            dataset=self.test_set,
            batch_size=self.test_batch_size,
            shuffle=False,
            pin_memory=self.pin_memory,
            drop_last=False,
            num_workers=self.num_workers,
            persistent_workers=True
        )

        
    